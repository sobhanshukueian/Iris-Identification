{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb9--mwA_E9O"
      },
      "source": [
        "## Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib\n",
        "# matplotlib.use(\"TKAgg\")\n",
        "import matplotlib.pyplot as plt\n",
        "# \n",
        "fig,ax=plt.subplots(1, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuHhXgs7jeFD"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib\n",
        "import os.path as osp\n",
        "\n",
        "%matplotlib inline\n",
        "import PIL.ImageOps    \n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.utils\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0xQoOPbjZeG"
      },
      "source": [
        "## Augment Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utuPIYZ-jBZy"
      },
      "outputs": [],
      "source": [
        "# # SP noise Augmentation\n",
        "# def sp_noise(image, prob):\n",
        "#     output = np.zeros((image.shape[0], image.shape[1],3),np.uint8)\n",
        "#     thres = 1 - prob \n",
        "#     for i in range(image.shape[0]):\n",
        "#         for j in range(image.shape[1]):\n",
        "#             rdn = random.random()\n",
        "\n",
        "#             if rdn < prob:\n",
        "\n",
        "#                 output[i][j][0] = 0\n",
        "#                 output[i][j][1] = 0\n",
        "#                 output[i][j][2] = 0\n",
        "\n",
        "\n",
        "#             elif rdn > thres:\n",
        "\n",
        "#                 output[i][j][0] = 255\n",
        "#                 output[i][j][1] = 255\n",
        "#                 output[i][j][2] = 255\n",
        "\n",
        "#             else:\n",
        "\n",
        "#                 output[i][j][0] = image[i][j][0]\n",
        "#                 output[i][j][1] = image[i][j][1]\n",
        "#                 output[i][j][2] = image[i][j][2]\n",
        "\n",
        "#     return output\n",
        "# # Gaussian noise Augmentaion\n",
        "# def gaussian_noise(image):\n",
        "#   gauss = np.random.normal(0 , 15, image.size,)\n",
        "#   gauss = gauss.reshape(image.shape[0], image.shape[1], image.shape[2])\n",
        "#   # image_hist(gauss)\n",
        "#   img_gauss = cv2.add(image, gauss, dtype=8)\n",
        "#   # image_hist(img_gauss)\n",
        "#   return img_gauss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "To9q7JweGKTo"
      },
      "outputs": [],
      "source": [
        "# !rm -rf \"/content/drive/MyDrive/Iris identification/train_augmented\"\n",
        "# !rm -rf \"/content/drive/MyDrive/Iris identification/val_augmented\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxd8Fc-EpZ30"
      },
      "outputs": [],
      "source": [
        "# DES_PATH_TRAIN = \"/content/drive/MyDrive/Iris identification/train_augmented/\"\n",
        "# DES_PATH_VAL = \"/content/drive/MyDrive/Iris identification/val_augmented/\"\n",
        "# DATA_PATH = \"/content/drive/MyDrive/Iris identification/normalized/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DES_PATH_TRAIN = \"./train_augmented/\"\n",
        "DES_PATH_VAL = \"./val_augmented/\"\n",
        "DATA_PATH = \"./normalized/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVhR5uGZkxf3"
      },
      "outputs": [],
      "source": [
        "# os.makedirs(DES_PATH_TRAIN)\n",
        "# os.makedirs(DES_PATH_VAL)\n",
        "\n",
        "# for img in os.listdir(DATA_PATH):\n",
        "#     tmp = 1\n",
        "#     image = cv2.imread(DATA_PATH + img)\n",
        "#     if int(img[:3]) < 40:\n",
        "#         image_dir = DES_PATH_TRAIN + str(int(img[:3]))+ f\"_{tmp}.jpg\"\n",
        "#         while osp.exists(image_dir):\n",
        "#             tmp += 1\n",
        "#             image_dir = DES_PATH_TRAIN + str(int(img[:3])) + f\"_{tmp}.jpg\" \n",
        "            \n",
        "#         cv2.imwrite(image_dir, image)\n",
        "#         cv2.imwrite(DES_PATH_TRAIN + str(int(img[:3])) + f\"_{tmp+1}.jpg\", sp_noise(image, 0.02))\n",
        "#         cv2.imwrite(DES_PATH_TRAIN + str(int(img[:3])) + f\"_{tmp+2}.jpg\", gaussian_noise(image))\n",
        "#     else: \n",
        "#         image_dir = DES_PATH_VAL + str(int(img[:3])) + f\"_{tmp}.jpg\"\n",
        "#         while osp.exists(image_dir):\n",
        "#             tmp += 1\n",
        "#             image_dir = DES_PATH_VAL + str(int(img[:3])) +f\"_{tmp}.jpg\" \n",
        "            \n",
        "#         cv2.imwrite(image_dir, image)\n",
        "#         cv2.imwrite(DES_PATH_VAL + str(int(img[:3])) + f\"_{tmp+1}.jpg\", sp_noise(image, 0.02))\n",
        "#         cv2.imwrite(DES_PATH_VAL + str(int(img[:3])) + f\"_{tmp+2}.jpg\", gaussian_noise(image))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32U2Yc0D_T_V"
      },
      "source": [
        "## Create Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxZCztnYIcii"
      },
      "outputs": [],
      "source": [
        "DES_PATH_TRAIN = \"./train_augmented/\"\n",
        "DES_PATH_VAL = \"./val_augmented/\"\n",
        "DATA_PATH = \"./normalized/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gs7s-gquJeF"
      },
      "outputs": [],
      "source": [
        "class SiameseNetworkDataset(Dataset):\n",
        "    def __init__(self, imageFolderDataset, size_multiplier, class_num=[1, 50], transform=None):\n",
        "        self.imageFolderDataset = imageFolderDataset    \n",
        "        self.transform = transform\n",
        "\n",
        "        self.pair1 = []\n",
        "        self.pair2 = []\n",
        "        self.labels = []\n",
        "        self.label1 = []\n",
        "        self.label2 = []\n",
        "\n",
        "        for _ in range(size_multiplier):\n",
        "            for i in range(class_num[0], class_num[1]):\n",
        "                for j in range(class_num[0], class_num[1]):\n",
        "                    if int(i) == int(j):\n",
        "                        y = 0\n",
        "                        c = class_num[1] - class_num[0] - 1\n",
        "                    else:\n",
        "                        y = 1\n",
        "                        c = 1\n",
        "                    for _ in range(c):\n",
        "                        self.label1.append(i)\n",
        "                        self.label2.append(j)\n",
        "                        self.pair1.append(np.random.randint(1, 19))\n",
        "                        self.pair2.append(np.random.randint(1, 19))\n",
        "                        self.labels.append(y)\n",
        "        self.randomize = np.arange(len(self.labels))\n",
        "        np.random.shuffle(self.randomize)    \n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        index = self.randomize[index]\n",
        "        label = self.labels[index]\n",
        "        label0 = self.label1[index]\n",
        "        label1 = self.label2[index]\n",
        "        img0 = self.imageFolderDataset + str(label0+1) + \"_\" + str(self.pair1[index]) + \".jpg\"\n",
        "        img1 = self.imageFolderDataset + str(label1+1) + \"_\" + str(self.pair2[index]) + \".jpg\"\n",
        "\n",
        "\n",
        "        img0 = Image.open(img0)\n",
        "        img1 = Image.open(img1)\n",
        "\n",
        "        img0 = img0.convert(\"L\")\n",
        "        img1 = img1.convert(\"L\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img0 = self.transform(img0)\n",
        "            img1 = self.transform(img1)\n",
        "        \n",
        "        return img0, img1, label, label0, label1\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fNJ5w_k-uaY"
      },
      "outputs": [],
      "source": [
        "transformation = transforms.Compose([transforms.ToTensor()])\n",
        "siamese_dataset = SiameseNetworkDataset(imageFolderDataset=DES_PATH_TRAIN, size_multiplier = 10, class_num=[1, 39], transform=transformation)\n",
        "siamese_testset = SiameseNetworkDataset(imageFolderDataset=DES_PATH_VAL, size_multiplier = 2, class_num=[40, 64], transform=transformation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfCJZr1U_hrB"
      },
      "outputs": [],
      "source": [
        "# Load the training dataset\n",
        "train_dataloader = DataLoader(siamese_dataset, shuffle=False, num_workers=0, batch_size=BATCH_SIZE)\n",
        "# Load the testing dataset\n",
        "test_dataloader = DataLoader(siamese_testset, shuffle=False, num_workers=0, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfvVMK_N-llC"
      },
      "outputs": [],
      "source": [
        "# Showing images\n",
        "def show_batch(data, cols = BATCH_SIZE):\n",
        "    fig, ax = plt.subplots(2, cols, figsize=(cols*10, 2*5))\n",
        "    for index in range(cols):\n",
        "        ax[0, index].imshow(data[0][index].view(90, 360), cmap='gray')\n",
        "        ax[1, index].imshow(data[1][index].view(90, 360), cmap='gray')\n",
        "    plt.show()\n",
        "    print(data[2].numpy().reshape(-1))\n",
        "    print(data[3].numpy().reshape(-1))\n",
        "    print(data[4].numpy().reshape(-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "EM2-rwJP_I9e",
        "outputId": "6adc3e54-5f60-43e5-af50-dc4b5620f56b"
      },
      "outputs": [],
      "source": [
        "# Create a simple dataloader just for simple visualization\n",
        "vis_dataloader = DataLoader(siamese_dataset, shuffle=True, num_workers=0, batch_size=10)\n",
        "\n",
        "# Extract one batch\n",
        "example_batch = next(iter(vis_dataloader))\n",
        "show_batch(example_batch, cols=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlDhsy7rAXp5"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPv-g7ccFbTH"
      },
      "outputs": [],
      "source": [
        "OUTPUT_EMBEDDING_SIZE = 16\n",
        "\n",
        "#create the Siamese Neural Network\n",
        "class SiameseNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "\n",
        "        # Setting up the Sequential of CNN Layers\n",
        "        self.cnn1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 5, kernel_size=(3, 9)),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.MaxPool2d(4),\n",
        "            nn.BatchNorm2d(5),\n",
        "            nn.Dropout(0.25),\n",
        "            \n",
        "            nn.Conv2d(5, 10, kernel_size=(3, 9)),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.Dropout(0.25),\n",
        "\n",
        "            nn.Conv2d(10, 15, kernel_size=(3, 9)),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.BatchNorm2d(15, affine=False),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Conv2d(15, OUTPUT_EMBEDDING_SIZE, kernel_size=(4, 16)),\n",
        "            nn.BatchNorm2d(OUTPUT_EMBEDDING_SIZE)\n",
        "\n",
        "        )\n",
        "        # self.cnn2 = nn.Conv2d(15, OUTPUT_EMBEDDING_SIZE, kernel_size=(4, 16))\n",
        "        self.f = nn.Flatten()\n",
        "        \n",
        "    def forward_once(self, x):\n",
        "        # This function will be called for both images\n",
        "        # Its output is used to determine the similiarity\n",
        "        output = self.cnn1(x)\n",
        "        output = torch.mul(output, 1.4142135623730950488)\n",
        "        # output = self.cnn2(output)\n",
        "        output = self.f(output)\n",
        "        return output\n",
        "\n",
        "    def forward(self, input1, input2):\n",
        "        # In this function we pass in both images and obtain both vectors\n",
        "        # which are returned\n",
        "        output1 = self.forward_once(input1)\n",
        "        output2 = self.forward_once(input2)\n",
        "\n",
        "        return output1, output2\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iC8hDpAAxlR"
      },
      "source": [
        "## Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFeBsGtkFdR_"
      },
      "outputs": [],
      "source": [
        "# Define the Contrastive Loss Function\n",
        "class ContrastiveLoss(torch.nn.Module):\n",
        "    def __init__(self, margin=2):\n",
        "        super(ContrastiveLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, output1, output2, label):\n",
        "        # Calculate the euclidean distance and calculate the contrastive loss\n",
        "        euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True).squeeze()\n",
        "        # print(euclidean_distance)\n",
        "        losses = (1-label) * torch.pow(euclidean_distance, 2) + (label) * 5 * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)    \n",
        "        loss_contrastive = torch.mean(losses)\n",
        "        # print(losses)\n",
        "        return loss_contrastive, losses, euclidean_distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdxjSE-DA3FJ"
      },
      "source": [
        "## Train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZsOLkPVA4ad"
      },
      "outputs": [],
      "source": [
        "def get_color(idx):\n",
        "    return list(matplotlib.colors.cnames.keys())[idx]\n",
        "  \n",
        "def get_colors(idxs):\n",
        "    res = []\n",
        "    for idx in idxs:\n",
        "      res.append(get_color(idx))\n",
        "    return res "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Kcl8rHU7A5x5",
        "outputId": "7d529523-d819-41da-ac9d-2f9654e56857"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "from copy import deepcopy\n",
        "import os.path as osp\n",
        "import shutil\n",
        "from prettytable import PrettyTable\n",
        "import json\n",
        "from sklearn.metrics import auc\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.cuda import amp\n",
        "# from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "class Trainer:\n",
        "    # -----------------------------------------------INITIALIZE TRAINING-------------------------------------------------------------\n",
        "    def __init__(self, device=device, epochs=10, batch_size=BATCH_SIZE, save_dir=\"./just_batch_runs\", train_loader=train_dataloader, valid_loader=test_dataloader, weights=None, verbose=3, visualize_plots=True, save_plots=True):\n",
        "        self.device = device\n",
        "        self.save_dir = save_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.use_ema = False\n",
        "        self.model_name = \"AT&T Face Identification and Simple Siamese Architecture that end of the model apply batch Normalization Then scale it.\"\n",
        "        self.weights = weights\n",
        "        self.visualize_plots = visualize_plots\n",
        "        self.save_plots = save_plots\n",
        "        # 0 == nothing || 1 == model architecture || 2 == print optimizer || 3 == model parameters\n",
        "        self.verbose = verbose\n",
        "        self.train_losses=[]\n",
        "        self.val_losses=[]\n",
        "        self.conf = {'Name' : self.model_name, 'Bacth_size' : self.batch_size, 'Max_iter_num' : '', 'Epochs' : self.epochs, 'Trained_epoch' : 0, 'Optimizer' : '', \"Model\" : '', 'Parameter_size' : ''}\n",
        "\n",
        "        temm=0\n",
        "        tmp_save_dir = self.save_dir\n",
        "        while osp.exists(tmp_save_dir):\n",
        "            tmp_save_dir = self.save_dir\n",
        "            temm+=1\n",
        "            tmp_save_dir += (str(temm))\n",
        "        self.save_dir = tmp_save_dir\n",
        "        del temm\n",
        "\n",
        "\n",
        "        # get data loader\n",
        "        self.train_loader, self.valid_loader = train_loader, valid_loader\n",
        "        self.max_stepnum = len(self.train_loader)\n",
        "        self.conf[\"Max_iter_num\"] = self.max_stepnum\n",
        "        \n",
        "        # get model \n",
        "        self.model = self.get_model()\n",
        "        if self.verbose > 2:\n",
        "            self.count_parameters()\n",
        "\n",
        "        # Get optimizer\n",
        "        self.optimizer = self.get_optimizer()\n",
        "    \n",
        "        # tensorboard\n",
        "        # self.tblogger = SummaryWriter(self.save_dir) \n",
        "\n",
        "# ----------------------------------------------------INITIALIZERS-------------------------------------------------------------------------\n",
        "    # Get Model \n",
        "    def get_model(self):\n",
        "        # Get Model Archutecture From Model Class.\n",
        "        model = SiameseNetwork().to(self.device)\n",
        "\n",
        "        # finetune if pretrained model is set\n",
        "        if self.weights:  \n",
        "            print(f'Loading state_dict from {self.weights} for fine-tuning...')\n",
        "            model.load_state_dict(torch.load(self.weights))\n",
        "        # Log Model\n",
        "        if self.verbose > 0:\n",
        "            print('Model: {}'.format(model))\n",
        "        self.conf[\"Model\"] = str(model)\n",
        "        return model\n",
        "\n",
        "    def get_optimizer(self, optimizer=\"Adam\", lr0=0.001, momentum=0.937):\n",
        "        assert optimizer == 'SGD' or 'Adam', 'ERROR: unknown optimizer, use SGD defaulted'\n",
        "        if optimizer == 'SGD':\n",
        "            optim = torch.optim.SGD(self.model.parameters(), lr=lr0, momentum=momentum, nesterov=True)\n",
        "        elif optimizer == 'Adam':\n",
        "            optim = torch.optim.Adam(self.model.parameters(), lr=lr0, betas=(momentum, 0.999))\n",
        "\n",
        "        if self.verbose > 1:\n",
        "            print(f\"{'optimizer:'} {type(optim).__name__}\")\n",
        "        self.conf['Optimizer'] = f\"{'optimizer:'} {type(optim).__name__}\"\n",
        "        return optim\n",
        "\n",
        "    # Loss Function Definition\n",
        "    def compute_loss(self, output1, output2, label):\n",
        "        criterion = ContrastiveLoss()\n",
        "        loss = criterion(output1, output2, label)\n",
        "        return loss\n",
        "\n",
        "    def count_parameters(self):\n",
        "        table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "        total_params = 0\n",
        "        for name, parameter in self.model.named_parameters():\n",
        "            if not parameter.requires_grad: continue\n",
        "            params = parameter.numel()\n",
        "            table.add_row([name, params])\n",
        "            total_params+=params\n",
        "        print(table)\n",
        "        print(f\"Total Trainable Params: {total_params}\")\n",
        "        self.conf[\"Parameter_size\"] = total_params\n",
        "# -------------------------------------------------------------------------------TRAINING PROCESS-----------------------------------------------\n",
        "    @staticmethod\n",
        "    def prepro_data(batch_data, device):\n",
        "        images1 = batch_data[0].to(device)\n",
        "        images2 = batch_data[1].to(device) \n",
        "        targets = batch_data[2].to(device)\n",
        "        images1_class = batch_data[3].to(device)\n",
        "        images2_class = batch_data[4].to(device)\n",
        "        return images1, images2, targets, images1_class, images2_class\n",
        "\n",
        "    # Each Train Step\n",
        "    def train_step(self, batch_data):\n",
        "        images1, images2, targets, images1_class, images2_class = self.prepro_data(batch_data, self.device)\n",
        "        # forward\n",
        "        with amp.autocast(enabled=self.device != 'cpu'):\n",
        "            preds = self.model(images1, images2)\n",
        "            loss, losses, dis = self.compute_loss(preds[0], preds[1], targets)\n",
        "        l2_lambda = 0.001\n",
        "        l2_norm = sum(p.pow(2.0).sum() for p in self.model.parameters())\n",
        "\n",
        "        loss = loss + l2_lambda * l2_norm\n",
        "        # backward\n",
        "        self.optimizer.zero_grad()\n",
        "        self.scaler.scale(loss).backward()\n",
        "        self.scaler.step(self.optimizer)\n",
        "        self.scaler.update()\n",
        "        \n",
        "        return loss.cpu().detach().numpy(), [pred.cpu().detach().numpy() for pred in preds], images1_class.cpu().detach().numpy(), images2_class.cpu().detach().numpy(), targets.cpu().detach().numpy()\n",
        "\n",
        "    # Each Validation Step\n",
        "    def val_step(self, batch_data):\n",
        "        self.model.eval()\n",
        "        images1, images2, targets, images1_class, images2_class = self.prepro_data(batch_data, self.device)\n",
        "\n",
        "        # forward\n",
        "        preds = self.model(images1, images2)\n",
        "        loss, losses, dis = self.compute_loss(preds[0], preds[1], targets)\n",
        "        return loss.cpu().detach().numpy(), [pred.cpu().detach().numpy() for pred in preds], images1_class.cpu().detach().numpy(), images2_class.cpu().detach().numpy(), targets.cpu().detach().numpy()\n",
        "\n",
        "    # Training Process\n",
        "    def train(self):\n",
        "        try:\n",
        "            # training process prerequisite\n",
        "            self.start_time = time.time()\n",
        "            print('Start Training Process \\nTime: {}'.format(time.ctime(self.start_time)))\n",
        "            self.scaler = amp.GradScaler(enabled=self.device != 'cpu')\n",
        "            self.best_loss = np.inf\n",
        "\n",
        "            # Epoch Loop\n",
        "            for self.epoch in range(0, self.epochs):\n",
        "                try:\n",
        "                    self.conf[\"Trained_epoch\"] = self.epoch\n",
        "\n",
        "                    # Training loop\n",
        "                    self.model.train(True)\n",
        "                    pbar = enumerate(self.train_loader)\n",
        "                    pbar = tqdm(pbar, total=self.max_stepnum)\n",
        "                    for step, batch_data in pbar:\n",
        "                        self.train_loss, embeds, images1_class, images2_class, targets = self.train_step(batch_data)\n",
        "                        self.train_losses.append(self.train_loss)\n",
        "                        pbar.set_description(f\"Epoch: {self.epoch}/{self.epochs}\\tTrain Loss: {self.train_loss}  \")\n",
        "                    del pbar\n",
        "\n",
        "                    # Train Validation Loop\n",
        "                    training1_embeddings = []\n",
        "                    training2_embeddings = []\n",
        "                    images1_classes = []\n",
        "                    images2_classes = []\n",
        "                    labels = []\n",
        "\n",
        "                    val_training1_embeddings = []\n",
        "                    val_training2_embeddings = []\n",
        "                    val_images1_classes = []\n",
        "                    val_images2_classes = []\n",
        "                    val_labels = []\n",
        "\n",
        "                    tbar = enumerate(self.train_loader)\n",
        "                    tbar = tqdm(tbar, total=self.max_stepnum)\n",
        "                    for step, batch_data in tbar:\n",
        "                        self.train_val_loss, embeds, images1_class, images2_class, targets = self.val_step(batch_data)\n",
        "                        tbar.set_description(f\"Epoch: {self.epoch}/{self.epochs}\\tTrain Validation Loss: {self.train_val_loss} \")\n",
        "                        training1_embeddings.extend(embeds[0])\n",
        "                        training2_embeddings.extend(embeds[1])\n",
        "                        images1_classes.extend(images1_class)\n",
        "                        images2_classes.extend(images2_class)\n",
        "                        labels.extend(targets)\n",
        "                    del tbar\n",
        "\n",
        "                    # Validation Loop\n",
        "                    vbar = enumerate(self.valid_loader)\n",
        "                    vbar = tqdm(vbar, total=len(self.valid_loader))\n",
        "                    for step, batch_data in vbar:\n",
        "                        self.val_loss, val_embeds, val_images1_class, val_images2_class, val_targets = self.val_step(batch_data)\n",
        "                        self.val_losses.append(self.val_loss)\n",
        "                        vbar.set_description(f\"Epoch: {self.epoch}/{self.epochs}\\tValidation Loss Loss: {self.val_loss}  \")\n",
        "                        val_training1_embeddings.extend(val_embeds[0])\n",
        "                        val_training2_embeddings.extend(val_embeds[1])\n",
        "                        val_images1_classes.extend(val_images1_class)\n",
        "                        val_images2_classes.extend(val_images2_class)\n",
        "                        val_labels.extend(val_targets)\n",
        "                    del vbar\n",
        "\n",
        "                    # PLot Losses\n",
        "                    self.plot_loss()\n",
        "\n",
        "                    # calculate euclidean distances\n",
        "                    euclidean_distance = np.linalg.norm((np.array(training1_embeddings) - np.array(training2_embeddings)), axis=1)\n",
        "                    val_euclidean_distance = np.linalg.norm((np.array(val_training1_embeddings) - np.array(val_training2_embeddings)), axis=1)\n",
        "\n",
        "                    # Evaluate \n",
        "                    # self.evaluation(euclidean_distance=euclidean_distance, labels=labels, title=\"Training\")\n",
        "                    self.evaluation(euclidean_distance=val_euclidean_distance, labels=val_labels, title=\"Validation\")\n",
        "\n",
        "                    # PLot Embeddings\n",
        "                    plot_size = BATCH_SIZE\n",
        "                    self.plot_embeddings(euclidean_distance, val_euclidean_distance, np.array(training1_embeddings), np.array(training2_embeddings), np.array(images1_classes), np.array(images2_classes), np.array(val_training1_embeddings), np.array(val_training2_embeddings), np.array(val_images1_classes), np.array(val_images2_classes), plot_size, plot_size)\n",
        "\n",
        "                    # Delete Data after PLotting\n",
        "                    del training1_embeddings, training2_embeddings, images1_classes, images2_classes, val_training1_embeddings, val_training2_embeddings, val_images1_classes, val_images2_classes\n",
        "                    \n",
        "                    if self.val_loss < self.best_loss:\n",
        "                        self.best_loss=self.val_loss\n",
        "\n",
        "                except Exception as _:\n",
        "                    print('ERROR in training steps.')\n",
        "                    raise\n",
        "                try:\n",
        "                    self.save()\n",
        "                except Exception as _:\n",
        "                    print('ERROR in evaluate and save model.')\n",
        "                    raise\n",
        "        except Exception as _:\n",
        "            print('ERROR in training loop or eval/save model.')\n",
        "            raise\n",
        "        finally:\n",
        "            finish_time = time.time()\n",
        "            print(f'\\nTraining completed in {time.ctime(finish_time)} \\nIts Done in: {(time.time() - self.start_time) / 3600:.3f} hours.') \n",
        "    \n",
        "\n",
        "    # -------------------------------------------------------Training Callback after each epoch--------------------------\n",
        "    def plot_loss(self, train_mean_size=1, val_mean_size=1):\n",
        "        COLS=3\n",
        "        ROWS=1\n",
        "        LINE_WIDTH = 2\n",
        "        fig, ax = plt.subplots(ROWS, COLS, figsize=(COLS*10, ROWS*10))\n",
        "        fig.suptitle(\"Losses Plot\", fontsize=16)\n",
        "\n",
        "        # train_mean_size = self.max_stepnum/self.batch_size\n",
        "        ax[0].plot(np.arange(len(self.train_losses) / train_mean_size), np.mean(np.array(self.train_losses).reshape(-1, train_mean_size), axis=1), 'r',  label=\"training loss\", linewidth=LINE_WIDTH)\n",
        "        ax[0].set_title(\"Training Loss\")\n",
        "\n",
        "        val_mean_size = len(self.valid_loader)\n",
        "        ax[1].plot(np.arange(len(self.val_losses) / val_mean_size), np.mean(np.array(self.val_losses).reshape(-1, val_mean_size), axis=1), 'g',  label=\"validation loss\", linewidth=LINE_WIDTH)\n",
        "        ax[1].set_title(\"Validation Loss\")\n",
        "\n",
        "        train_mean_size = self.max_stepnum\n",
        "        ax[2].plot(np.arange(len(self.train_losses) / train_mean_size), np.mean(np.array(self.train_losses).reshape(-1, train_mean_size), axis=1), 'r',  label=\"training loss\", linewidth=LINE_WIDTH)\n",
        "        ax[2].plot(np.arange(len(self.val_losses) / val_mean_size), np.mean(np.array(self.val_losses).reshape(-1, val_mean_size), axis=1), 'g',  label=\"validation loss\", linewidth=LINE_WIDTH)\n",
        "        ax[2].set_title(\"Train Validation Loss\")\n",
        "\n",
        "        if self.save_plots:\n",
        "            save_plot_dir = osp.join(self.save_dir, 'plots') \n",
        "            if not osp.exists(save_plot_dir):\n",
        "                os.makedirs(save_plot_dir)\n",
        "            plt.savefig(\"{}/epoch-{}-loss-plot.png\".format(save_plot_dir, self.epoch)) \n",
        "        if self.visualize_plots:\n",
        "            plt.show()\n",
        "\n",
        "    def plot_embeddings(self, euclidean_distance, val_euclidean_distance, training1_embeddings, training2_embeddings, images1_classes, images2_classes, val_training1_embeddings, val_training2_embeddings, val_images1_classes, val_images2_classes, train_plot_size=0, val_plot_size=0):\n",
        "        if train_plot_size > 0:\n",
        "            training1_embeddings = np.array(training1_embeddings[:train_plot_size])\n",
        "            training2_embeddings = np.array(training2_embeddings[:train_plot_size])\n",
        "            images1_classes = np.array(images1_classes[:train_plot_size])\n",
        "            images2_classes = np.array(images2_classes[:train_plot_size])\n",
        "        if val_plot_size > 0:\n",
        "            val_training1_embeddings = np.array(val_training1_embeddings[:val_plot_size])\n",
        "            val_training2_embeddings = np.array(val_training2_embeddings[:val_plot_size])\n",
        "            val_images1_classes = np.array(val_images1_classes[:val_plot_size])\n",
        "            val_images2_classes = np.array(val_images2_classes[:val_plot_size])\n",
        "        # print(\"Train embedding size : {} and val_training embedding size : {} image1_class size : {} image2_class size : {}\".format(len(training1_embeddings), len(val_training1_embeddings), len(images1_classes), len(images2_classes)))\n",
        "        # print(training1_embeddings, training1_embeddings[:, 0])\n",
        "\n",
        "        COLS = 3\n",
        "        ROWS = int(OUTPUT_EMBEDDING_SIZE / 2) + 1\n",
        "        fig, ax = plt.subplots(ROWS, COLS, figsize=(COLS*10, ROWS*10))\n",
        "        # fig.suptitle(\"Embeddings Plot\", fontsize=16)\n",
        "        for dim in range(0, OUTPUT_EMBEDDING_SIZE, 2):\n",
        "            ax[int(dim/2), 0].set_title(\"Training Embeddings for {} and {} dimensions\".format(dim, dim+1))\n",
        "            ax[int(dim/2), 0].scatter(training1_embeddings[:, dim], training1_embeddings[:, dim+1], c=get_colors(images1_classes))\n",
        "            ax[int(dim/2), 0].scatter(training2_embeddings[:, dim], training2_embeddings[:, dim+1], c=get_colors(images2_classes))\n",
        "\n",
        "            ax[int(dim/2), 1].set_title(\"Validation Embeddings for {} and {} dimensions\".format(dim, dim+1))\n",
        "            ax[int(dim/2), 1].scatter(val_training1_embeddings[:, dim], val_training1_embeddings[:, dim+1], c=get_colors(val_images1_classes))\n",
        "            ax[int(dim/2), 1].scatter(val_training2_embeddings[:, dim], val_training2_embeddings[:, dim+1], c=get_colors(val_images2_classes))\n",
        "\n",
        "            \n",
        "            ax[int(dim/2), 2].set_title(\"Train Val for {} and {} dimensions\".format(dim , dim+1))\n",
        "            ax[int(dim/2), 2].scatter(training1_embeddings[:, dim], training1_embeddings[:, dim+1], c=get_colors(images1_classes))\n",
        "            ax[int(dim/2), 2].scatter(training2_embeddings[:, dim], training2_embeddings[:, dim+1], c=get_colors(images2_classes))\n",
        "            ax[int(dim/2), 2].scatter(val_training1_embeddings[:, dim], val_training1_embeddings[:, dim+1], c=get_colors(val_images1_classes))\n",
        "            ax[int(dim/2), 2].scatter(val_training2_embeddings[:, dim], val_training2_embeddings[:, dim+1], c=get_colors(val_images2_classes))\n",
        "    \n",
        "        # Draw Rectangle\n",
        "            rect_minx = min(np.min(val_training1_embeddings[:, dim]), np.min(val_training2_embeddings[:, dim]))\n",
        "            rect_maxx = max(np.max(val_training1_embeddings[:, dim]), np.max(val_training2_embeddings[:, dim]))\n",
        "            rect_miny = min(np.min(val_training1_embeddings[:, dim+1]), np.min(val_training2_embeddings[:, dim+1]))\n",
        "            rect_maxy = max(np.max(val_training1_embeddings[:, dim+1]), np.max(val_training2_embeddings[:, dim+1]))\n",
        "            rect=matplotlib.patches.Rectangle(xy=(rect_minx, rect_miny), width=rect_maxx-rect_minx, height=rect_maxy-rect_miny, linewidth=1, edgecolor='r', facecolor='none')\n",
        "            ax[int(dim/2), 2].add_patch(rect)\n",
        "\n",
        "        # Draw Histogram\n",
        "        pos=[]\n",
        "        neg=[]\n",
        "        for index, label in enumerate(images1_classes):\n",
        "            if label == images2_classes[index]:\n",
        "                pos.append(euclidean_distance[index])\n",
        "            elif label != images2_classes[index]:\n",
        "                neg.append(euclidean_distance[index])\n",
        "\n",
        "        ax[int(OUTPUT_EMBEDDING_SIZE / 2), 0].set_title(\"Training Distances\")\n",
        "        ax[int(OUTPUT_EMBEDDING_SIZE / 2), 0].hist(pos, bins = 15,  alpha = 0.5, lw=3, color='g', range=(0, np.max(euclidean_distance)), label=\"Positive Pairs\")\n",
        "        ax[int(OUTPUT_EMBEDDING_SIZE / 2), 0].hist(neg, bins = 15,  alpha = 0.5, lw=3, color='r', range=(0, np.max(euclidean_distance)), label=\"Negative Pairs\")\n",
        "        ax[int(OUTPUT_EMBEDDING_SIZE / 2), 0].legend()\n",
        "\n",
        "\n",
        "        val_pos=[]\n",
        "        val_neg=[]\n",
        "        for index, label in enumerate(val_images1_classes):\n",
        "            if label == val_images2_classes[index]:\n",
        "                val_pos.append(val_euclidean_distance[index])\n",
        "            elif label != val_images2_classes[index]:\n",
        "                val_neg.append(val_euclidean_distance[index])\n",
        "\n",
        "        ax[int(OUTPUT_EMBEDDING_SIZE / 2), 1].set_title(\"Validation Distances\")\n",
        "        ax[int(OUTPUT_EMBEDDING_SIZE / 2), 1].hist(val_pos, bins = 15, alpha = 0.5, lw=3, color='g', range=(0, np.max(val_euclidean_distance)), label=\"Positive Pairs\")\n",
        "        ax[int(OUTPUT_EMBEDDING_SIZE / 2), 1].hist(val_neg, bins = 15, alpha = 0.5, lw=3, color='r', range=(0, np.max(val_euclidean_distance)), label=\"Negative Pairs\")\n",
        "        ax[int(OUTPUT_EMBEDDING_SIZE / 2), 1].legend()\n",
        "\n",
        "        ax[int(OUTPUT_EMBEDDING_SIZE / 2), 2].set_title(\"Train Val Distances\")\n",
        "        ax[int(OUTPUT_EMBEDDING_SIZE / 2), 2].hist(np.concatenate([pos, val_pos]), bins = 15, alpha = 0.5, lw=3, color='g', range=(0, max(np.max(euclidean_distance), np.max(val_euclidean_distance))), label=\"Positive Pairs\")\n",
        "        ax[int(OUTPUT_EMBEDDING_SIZE / 2), 2].hist(np.concatenate([neg, val_neg]), bins = 15, alpha = 0.5, lw=3, color='r', range=(0, max(np.max(euclidean_distance), np.max(val_euclidean_distance))), label=\"Negative Pairs\")\n",
        "        ax[int(OUTPUT_EMBEDDING_SIZE / 2), 2].legend()\n",
        "\n",
        "        if self.save_plots:\n",
        "            save_plot_dir = osp.join(self.save_dir, 'plots') \n",
        "            if not osp.exists(save_plot_dir):\n",
        "                os.makedirs(save_plot_dir)\n",
        "            plt.savefig(\"{}/epoch-{}-plot.png\".format(save_plot_dir, self.epoch)) \n",
        "        if self.visualize_plots:\n",
        "            plt.show()\n",
        "\n",
        "    def evaluation(self, euclidean_distance, labels, title):\n",
        "        true_pose_rate = []\n",
        "        false_pose_rate = []\n",
        "        precisions = []\n",
        "        recalls = []\n",
        "        f1_scores = []\n",
        "        thresholds = []\n",
        "        accs = []\n",
        "\n",
        "        sorted_distances = np.sort(np.unique(euclidean_distance))\n",
        "\n",
        "        for index, dis in enumerate(sorted_distances):\n",
        "            if index == 0:\n",
        "                threshold = -1.0\n",
        "                thresholds.append(threshold)\n",
        "            else:\n",
        "                threshold = (dis + sorted_distances[index-1]) / 2.0\n",
        "                thresholds.append(threshold)\n",
        "            true_pos = 0\n",
        "            false_pos = 0\n",
        "            true_neg = 0\n",
        "            false_neg = 0\n",
        "            for i, label in enumerate(labels):\n",
        "                if label == 0:\n",
        "                    if euclidean_distance[i] < threshold:\n",
        "                        true_pos += 1\n",
        "                    else:\n",
        "                        false_neg += 1\n",
        "                elif label == 1:\n",
        "                    if euclidean_distance[i] > threshold:\n",
        "                        true_neg += 1\n",
        "                    else:\n",
        "                        false_pos += 1\n",
        "            \n",
        "            true_pose_rate.append(true_pos / (true_pos + false_neg))\n",
        "            false_pose_rate.append(false_pos / (true_neg + false_pos))\n",
        "\n",
        "            acc = (true_pos + true_neg) / (true_neg + false_neg + true_pos + false_pos)\n",
        "            precision = 1 if true_pos + false_pos == 0 else true_pos / (true_pos + false_pos)\n",
        "            recall = true_pos / (true_pos + false_neg)\n",
        "            f1_score = 2 * (recall * precision) / (recall + precision  + 0.000001)\n",
        "            precisions.append(precision)\n",
        "            recalls.append(recall)\n",
        "            f1_scores.append(f1_score)\n",
        "            accs.append(acc)\n",
        "\n",
        "        # print(thresholds)\n",
        "        # print(true_pose_rate)\n",
        "        # print(false_pose_rate)\n",
        "\n",
        "        # idx = np.argsort(np.array(false_pose_rate))\n",
        "        # false_pose_rate = np.array(false_pose_rate)[idx]\n",
        "        # true_pose_rate = np.array(true_pose_rate)[idx]\n",
        "        roc_auc = auc(false_pose_rate, true_pose_rate)\n",
        "        \n",
        "        COLS = 3\n",
        "        ROWS = 1\n",
        "        fig, ax = plt.subplots(ROWS, COLS, figsize=(COLS*10, ROWS*10))\n",
        "        fig.suptitle(title, fontsize=16)\n",
        "        ax[0].plot(false_pose_rate, true_pose_rate)\n",
        "        ax[0].plot([0, 1])\n",
        "        ax[0].set_xlabel('false positive')\n",
        "        ax[0].set_ylabel('True positive')\n",
        "        ax[0].set_title(\"ROC curve\\nAUC: {}\".format(roc_auc))\n",
        "        # print(\"AUC: {}\".format())\n",
        "\n",
        "        ax[1].plot(precisions, recalls)\n",
        "        ax[1].plot([0, 1])\n",
        "        ax[1].set_xlabel('Precisions')\n",
        "        ax[1].set_ylabel('Recalls')\n",
        "        ax[1].set_title(\"Precision/Recall curve\")\n",
        "        # print(\"AUC : {}\".format(self.calc_area(false_pose_rate, true_pose_rate)))\n",
        "\n",
        "\n",
        "        ax[2].plot(thresholds, accs)\n",
        "        ax[2].set_xlabel('Threshodls')\n",
        "        ax[2].set_ylabel('Accuracies')\n",
        "        ax[2].axvline(x=thresholds[np.argmax(accs)])\n",
        "        ax[2].set_title(\"Accuracies/Threshodls\\nMax: {}\".format(thresholds[np.argmax(accs)]))\n",
        "        if self.save_plots:\n",
        "            save_plot_dir = osp.join(self.save_dir, 'evaluation') \n",
        "            if not osp.exists(save_plot_dir):\n",
        "                os.makedirs(save_plot_dir)\n",
        "            plt.savefig(\"{}/epoch-{}-plot.png\".format(save_plot_dir, self.epoch)) \n",
        "        if self.visualize_plots:\n",
        "            plt.show()\n",
        "        return roc_auc\n",
        "    \n",
        "    # -------------------------------------------------------save Model-------------------------------------------\n",
        "    def save(self):\n",
        "        # create config object\n",
        "        conf = json.dumps(self.conf)\n",
        "        f = open(self.save_dir + \"/config.json\",\"w\")\n",
        "        f.write(conf)\n",
        "        f.close()\n",
        "        # save model\n",
        "        save_ckpt_dir = osp.join(self.save_dir, 'weights')\n",
        "        if not osp.exists(save_ckpt_dir):\n",
        "            os.makedirs(save_ckpt_dir)\n",
        "        filename = osp.join(save_ckpt_dir, self.model_name + \"-\" + str(self.epoch) + '.pt')\n",
        "        torch.save(self.model.state_dict(), filename)\n",
        "        if self.val_loss == self.best_loss:\n",
        "            best_filename = osp.join(save_ckpt_dir, 'best_{}.pt'.format(self.model_name, self.epoch))\n",
        "            if osp.exists(best_filename):\n",
        "                os.remove(best_filename)\n",
        "            shutil.copyfile(filename, best_filename)\n",
        "\n",
        "    # ----------------------------------------------------Run Model---------------------\n",
        "    def run(self, input1, input2):\n",
        "        self.model.eval()\n",
        "        img0 = Image.open(input1)\n",
        "        img1 = Image.open(input2)\n",
        "\n",
        "        img0 = img0.convert(\"L\")\n",
        "        img1 = img1.convert(\"L\")\n",
        "\n",
        "        image_tensor1 = transformation(img0).div_(255).float()\n",
        "        image_tensor2 = transformation(img1).div_(255).float()\n",
        "\n",
        "        fig, ax = plt.subplots(1,2)\n",
        "        ax[0].imshow(img0)\n",
        "        ax[1].imshow(img1)\n",
        "        plt.show()\n",
        "        \n",
        "        preds = self.model(image_tensor1.unsqueeze(0), image_tensor2.unsqueeze(0))\n",
        "        euclidean_distance = F.pairwise_distance(preds[0], preds[1], keepdim = True)\n",
        "        print(preds, euclidean_distance)\n",
        "\n",
        "        if euclidean_distance > 0.1:\n",
        "            print(\"Different Person Images Identified!\")\n",
        "        else: \n",
        "            print(\"Same Person Images Identified!\")\n",
        "\n",
        "Trainer(batch_size=BATCH_SIZE, device=device, epochs=100, verbose=10).train()\n",
        "# Trainer(batch_size=32, device=\"cpu\", epochs=50, verbose=0, weights=\"/content/runs/weights/best_SSL_epoch_45.pt\").run(\"/content/data/faces/testing/s5/2.pgm\", \"/content/data/faces/testing/s7/4.pgm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vA_o6dpJrdth"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Identification.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('torch-env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "f5a54485ec8884a421dc76c1a45488cad683eb04f8e7b906fbfcbf0983d61647"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
